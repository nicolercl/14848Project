# 14848 Final Project Option 1: Big Data Processing Toolbox - Checkpoint 1

## Docker Images & steps to run them on local machine

This section documents how I customized and ran the docker images in docker on the local machine before deploying to kubernetes. Skip this and go to the `Configuring images for Google Kubernetes Engine` section for steps to deploy on GKE.

### Apache Spark
- Base image: `bitnami/spark`.
- Run image with the command: `docker run -p 8080:8080 bitnami/spark`, then navigate to `localhost:8080` in the brower. It will display the PySpark webgui.

### Apache Hadoop
- Base images: 
    ```
    bde2020/hadoop-namenode
    bde2020/hadoop-datanode
    ```
- The original repository bde2020 uses `docker-compose up -d` command to run the images specified in their `docker-compose.yml` file as separate containers.
- To run on GKE, we can't use docker-compose, so we must be able to run them separately on the local machine too. Also, we should include all environment variables in the docker image (from the hadoop.env file in the original repository). So, I modified the original Dockerfile and created the new images:
    ```
    wwweiweiii/hadoop-namenode
    wwweiweiii/hadoop-datanode
    ```
- To run them in docker, the commands are:
    ```
    docker network create hadoop-net
    docker run -it --name namenode --net hadoop-net -p 9870:9870 -p 9000:9000 wwweiweiii/hadoop-namenode
    docker run -it --name datanode1 --net hadoop-net -p 9864:9864 wwweiweiii/hadoop-datanode
    docker run -it --name datanode2 --net hadoop-net -p 9865:9864 wwweiweiii/hadoop-datanode
    ```
- It's important to specify the `--name` of the container so that the datanodes can find the `namenode` container. Navigate to `localhost:9870` and it will show the Hadoop GUI with 2 datanodes.
![](https://i.imgur.com/o76vOio.png)

### Jupyter Notebook
- Base image: `jupyter/datascience-notebook`. For GKE deployment, I built on top of the base image to get `wwweiweiii/jupyter-notebook`.
- To run locally, run the base image with `docker run -p 8888:8888 jupyter/datascience-notebook`. There will be instructions to navigate to address in browser such as in the pic below. 
![](https://i.imgur.com/O0fEdfO.png)
Then, navigate to `localhost:8888?token=7726890a1b8e39bcca765ec1b696e04ee3a225cf70e2966c` in the browser to access the notebook. 
- To bypass the token security, I alternatively used the command `docker run -it -p 8888:8888 jupyter/datascience-notebook start.sh jupyter notebook --NotebookApp.token=''`. Then we can access the notebook by `localhost:8888`. I incorporated this command into the new image, `wwweiweiii/jupyter-notebook`.

### Sonarqube and Sonarscanner
- Base image: `wwweiweiii/sonarqubescanner`. This image is built upon the official `sonarqube` image which is of Alpine distribution. Therefore, to install sonar-scanner in the image, I included the commands below in the Dockerfile. The complete Dockerfile can be found in the `sonarqube` directory.
    ```
    RUN apk update && apk add npm && mkdir downloads/sonarqube -p && cd downloads/sonarqube/ && \
        wget https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-3.3.0.1492.zip && \
        unzip sonar-scanner-cli-3.3.0.1492.zip && mv sonar-scanner-3.3.0.1492/ /opt/sonar-scanner 
    ENV PATH $PATH:/opt/sonar-scanner/bin
    ```
- First, run the image via `docker run -it -p 9000:9000 -p 9092:9092 wwweiweiii/sonarqubescanner`. This takes a while to complete. When the terminal shows "Sonarqube is up", proceed to next step.
- Navigate to `localhost:9000` an the server will be up. Log in with `username: admin, password: admin` and reset password to access the GUI. Generate a token for a project from the Security tab in the GUI.
- To test sonar-scanner, open up an interactive shell to the container with `docker exec -it [Container ID]` and use the token generated by sonarqube web GUI in the sonar-scanner command: `sonar-scanner -X -Dsonar.projectKey=project -Dsonar.sources=. -Dsonar.host.url=http://127.0.0.1:9000 -Dsonar.login=[TOKEN]`.

*Alternatively, I wrote a bash script scanner.sh which is located in /usr/src/myapp in the container to reset password and generate the sonarqube project token using the web api. This bypasses the need for the user to access the terminal. It scans the current working directoy. When run locally, change the $SERVER_URL to 127.0.0.1:9000. Other installations such as python3 and curl are required (included in the new image already).*
```
# scanner.sh
export PASSWORD="secret"
curl -X POST -H "Content-Type: application/x-www-form-urlencoded" -d "login=admin" -d "password=$PASSWORD" -d "previousPassword=admin" -u admin:admin 127.0.0.1:9000/api/users/change_password
export SONAR_LOGIN=$(curl -X POST -H "Content-Type: application/x-www-form-urlencoded" -d "name=admin" -u admin:admin 127.0.0.1:9000/api/user_tokens/generate | python3 -c "import sys, json; print(json.loads(sys.stdin)["token"])")
sonar-scanner -X -Dsonar.projectKey=project -Dsonar.sources=. -Dsonar.host.url=http://127.0.0.1:9000 -Dsonar.login=$SONAR_LOGIN
```

### Terminal App
- Base image: `wwweiweiii/app`. This is built upon the `nginx` image and runs a static web page to prompt for selection of service and open the corresponding microservice in a new tab in the browser.
- To run locally, we need to use localhost and the ports above for each running microservice. So after running the 4 containers, we change the urls in entry/index.html for the 4 services. (Ex: for spark, update link to http://localhost:8080) Finally, rebuild the image with `docker -tag [New Image Name] entry/`.
- Command to run: `docker run -it -p 80:80 [New Image Name]`, then open the browser to `localhost:80`, where the updated links will work for the services.

## Configuring images for Google Kubernetes Engine

To deploy on GKE, I used the images:
```
wwweiweiii/hadoop-namenode
wwweiweiii/hadoop-datanode
wwweiweiii/jupyter-notebook
bitnami/spark
wwweiweiii/sonarqubescanner
postgresql:10
wwweiweiii/app
```
Note that we need to deploy and expose the 4 big data processing services first, get their IPs and update the entry app links before deploying the entry app.

The first step is to create a GKE cluster. 
1. Go to Kubernetes Engine -> Clusters -> Create.
2. Choose the GKE Standard option
3. Name the cluster `cluster-bdp`, and select us-east1-b as Zone. For default node locations, select us-east1-c as the additional node location.
*Note: I hard-coded the zones in the postgresql deployment yaml file, so change the yaml file region if the cluster is set in other zones here.*
![](https://i.imgur.com/XtPk2Oe.png)
4. Create the cluster. A screenshot of the running cluster:
![](https://i.imgur.com/OzZ3VXr.png)

### Single Container microservices

For single container microservices `jupter-notebook` and `spark`, I deployed them onto a GKE cluster `cluster-bdp` with the following steps:
1. Upload images to Google Container Registry. Do this for both`wwweiweiii/jupyter-notebook` and `bitnami/spark`
    ```
    # In cloud shell

    docker pull [IMAGE]
    docker tag [IMAGE] gcr.io/big-data-processing-toolbox/[IMAGE]
    docker push gcr.io/big-data-processing-toolbox/[IMAGE]
    ```
2. Deploy the containers
For `jupyter-notebook` and `spark`, go to the container registry and find the images, then choose Deploy to GKE in the dropdown. Set the container names as `jupyter-notebook` and `spark`, respectively.
![](https://i.imgur.com/68PDo51.png)
![](https://i.imgur.com/84rwPhX.png)
- Screenshot of deployments
![](https://i.imgur.com/evstmym.png)


3. Expose containers as services
Go to the individual workload page of the 2 containeres above in GKE workloads tab. Choose `Expose`, where we can expose them as services. I exposed them as NodePort type so that they can be accessible outside of the cluster by `[NodeIP:NodePort]`, where the node port is the same on every node for a service.
    
    For jupyter-notebook, set both Port & Target port to 8888. For spark, set both Port & Target port to 8080.
    
    ![](https://i.imgur.com/3ZrXRec.png)
- Screenshot of services
    ![](https://i.imgur.com/pYnnt0e.png)


4. Set up firewall rules to use ports 30000 and above (for NodePort services)
Since nodeports are 30000 and above, we need to set the firewall rules as below in the cloud shell for each service. The exposed node ports can be obtained in the NodePort column in the service page. In this example, the Jupyter node port is 31438, and the spark node port is 30489.
- Screenshot of Jupyter node port
![](https://i.imgur.com/8jMQ5mg.png)

    ```
    gcloud compute firewall-rules create spark-service --allow tcp:[Spark node port]
    gcloud compute firewall-rules create jupyter-notebook-service --allow tcp:[jupyter node port]
    ```
- Screenshot of cloud shell
    ![](https://i.imgur.com/eF2G7Hn.png)

5. Get Node IPs
Go to cloud shell and execute the command `kubectl get nodes -o wide`, which will show the nodes available and their IPs. We can use any available external IP with NodePort to access a service.
![](https://i.imgur.com/lRfzIJG.png)

6. By navigating to `[NodeIP:NodePort]`, we will be able to access the services.  We will use these links in the entry point app.
![](https://i.imgur.com/jtniL1l.png)
![](https://i.imgur.com/2z2Xn9n.png)

### Multi-container microservices

#### Hadoop 
For `hadoop`, we need to deploy a hadoop service with one master and two worker nodes, so we need a namenode container and two datanode containers.

Steps to deploy to GKE:
1. Upload images `wwweiweiii/hadoop-namenode` and `wwweiweiii/hadoop-datanode` to Container Registry (same commands as Single Container microservices section)
2. Deploy container: deploy 1 namenode container and 2 datanode containers, name them `namenode`, `datanode1` and `datanode2`.
![](https://i.imgur.com/TLYgJ8s.png)

3. Expose containers as services
Go to the individual workload page of the 3 containeres above in GKE workloads tab. Choose `Expose` as NodePort service for all 3. For namenode, set the service name to `namenode`, and 2 port mappings, Port & Target port = 9870 and Port & Target port = 9000. For datanode1 and datanode2, set their service names to `datanode1` and `datanode2`, respectively and port mapping to Port & Target port = 9864.
![](https://i.imgur.com/0LDO3fV.png)

4. Set up firewall rules to use ports 30000 and above (for NodePort services) for namenode 9870 port, since that is where we will access the Hadoop web gui. In my case, the node port is namenode: 31164.
    ```
    gcloud compute firewall-rules create namenode --allow tcp:[namenode Node port 1]
    ```
    ![](https://i.imgur.com/VJki5gg.png)

5. By navigating to `[NodeIP:namenode NodePort]`, we will be able to access the service.  This link will be used in the entry point app.
*Note: Since there are 3 pods running each datanode service, namenode might capture more than 1 instance of a datanode service, such as in the image below.*
![](https://i.imgur.com/O4XQpkV.png)


#### Sonarqube + Sonarscanner
For `sonarqubescanner`, we need to use persistent volume to run it successfully. Specifically, we connect it to a postgressql database container which requires its own persistent storage. Sonarqube also requires persistent storage for its `/opt/sonarqube/data/` and `/opt/sonarqube/extensions/` directories. If we don't use persistent volumes, we will be redirected to the login page when we try to log in to sonarqube. 

Steps to deploy to GKE:
1. Upload `wwweiweiii/sonarqubescanner` image to Container Registry (same commands as Single Container microservices section)
2. Clone this repository into cloud shell to use the yaml files located in `Big-Data-Processing-Toolbox/sonarqube`. Go to the sonarqube directory.
3. Create persistentVolumeClaim for postgresql: `kubectl apply -f postgres-pv.yaml`
4. Set up postgresql deployment: `kubectl apply -f postgres-deployment.yaml`
5. Set up postgresql service: `kubectl apply -f postgres-service.yaml`
6. Create persistentVolumeClaim for sonar data: `kubectl apply -f sonar-data.yaml`
7. Create persistentVolumeClaim for sonar extensions: `kubectl apply -f sonar-ext.yaml`
8. Create Sonarqube + scanner deployment: `kubectl apply -f sonar-deployment.yaml`
9. Expose sonar as NodePort service: `kubectl apply -f sonar-service.yaml`
- Screenshot of commands
![](https://i.imgur.com/y8bLyDy.png)
![](https://i.imgur.com/4RbwQof.png)

- Screenshot of volume storage
![](https://i.imgur.com/5XeRJLV.png)

- Screenshot of sonarqube+scanner and postgresql deployments
![](https://i.imgur.com/jc9BIi7.png)

- Screenshot of sonarqube+scanner and postgresql services
![](https://i.imgur.com/TZu6TsY.png)


10. Set up firewall rules to use ports 30000 and above (for NodePort services). In my case, the node port of sonarqubescanner is 31424.
    ```
    gcloud compute firewall-rules create sonarqube --allow tcp:[sonarqube nodeport]
    ```
    ![](https://i.imgur.com/YyWEd0z.png)

11. Navigate to [Node IP]:[NodePort] to use Sonarqube. Log in with username:admin and password: admin to start using.
![](https://i.imgur.com/7Eezxn6.png)

#### Entry point app

For the entry point app, I used an HTML file hosted on an NGINX server. It contains the welcoming message and links to the four microservices in the browser, repectively.

Steps to build app image:
1. After configuring and deploying all 4 microservices onto GKE, we can get the links to all the services and update the links in the entry point HTML: `entry/index.html`.
![](https://i.imgur.com/JCdQcSC.png)

2. Rebuild image with `docker build -t [New Image] /entry`, and push to Docker Hub with `docker push [New Image]`.

Steps to deploy on GKE:
1. Upload image to Container Registry (same commands as Single Container microservices section), but use the `[New Image]` name. 
2. Deploy container with name `bdp-app`
![](https://i.imgur.com/GTq5e2t.png)

4. Expose service as load-balancer
    Different from the other microservices, the entry of the app is deployed as a `load-balancer` type so that there is a static external IP to access the app.
![](https://i.imgur.com/fsLnfWN.png)

4. Navigating to the external IP, we can access the entry-point page.
![](https://i.imgur.com/zUGkwyp.png)
5. Clicking on the links, we can access each microservice in a new tab.
![](https://i.imgur.com/rgZ9kJf.png)
![](https://i.imgur.com/zVep4Qc.png)
![](https://i.imgur.com/Ut2eSji.png)
![](https://i.imgur.com/glHI867.png)
